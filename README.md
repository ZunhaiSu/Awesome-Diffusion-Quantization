# PTQ for Diffusion Transformers
|Paper|Conference|Author|Code|
|:---:|:---:|:---:|:---:|
|[TR-DQ: Time-Rotation Diffusion Quantization](https://arxiv.org/abs/2503.06564)|ICCV2025 under review|PKU||
|[TaQ-DiT: Time-aware Quantization for Diffusion Transformers](https://arxiv.org/abs/2411.14172)||Nanjing University|https://github.com/yhwangs/TQ-DiT|
|[PTQ4DiT: Post-training Quantization for Diffusion Transformers](https://arxiv.org/abs/2405.16005)|Neurips 2024|University of Illinois Chicago|https://github.com/adreamwu/PTQ4DiT|
|[Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers](https://arxiv.org/abs/2406.17343)|CVPR 2025|Tsinghua University|https://github.com/Juanerx/Q-DiT|
|[DiTAS: Quantizing Diffusion Transformers via Enhanced Activation Smoothing](https://arxiv.org/abs/2409.07756)|WACV 2025|New York University|https://github.com/DZY122/DiTAS|
|[ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation](https://arxiv.org/abs/2406.02540)|ICLR 2025|Tsinghua University|https://github.com/thu-nics/ViDiT-Q|
|[HQ-DiT: Efficient Diffusion Transformer with FP4 Hybrid Quantization](https://arxiv.org/abs/2405.19751)||New York University||
|[VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers](https://arxiv.org/abs/2408.17131)||Zhejiang University||
|[An Analysis on Quantizing Diffusion Transformers](https://arxiv.org/abs/2406.11100)||Meta GenAI||
|[Post-Training Quantization for Diffusion Transformer via Hierarchical Timestep Grouping](https://arxiv.org/pdf/2503.06930)||Peking University||
|[TQ-DiT: Efficient Time-Aware Quantization for Diffusion Transformers](https://arxiv.org/pdf/2502.04056)||KAIST||
|[FP4DiT: Towards Effective Floating Point Quantization for Diffusion Transformers](https://arxiv.org/pdf/2503.15465)||University of Alberta|https://github.com/cccrrrccc/FP4DiT|



# PTQ for Latent Diffusion Models
|Paper|Conference|Author|Code|
|:---:|:---:|:---:|:---:|
|||||
